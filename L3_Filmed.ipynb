{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Automating Model-Graded Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the API keys for our 3rd party APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_circle_api_key\n",
    "cci_api_key = get_circle_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_gh_api_key\n",
    "gh_api_key = get_gh_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_openai_api_key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our github branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mfunaki-circleci/llmops-course2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_repo_name\n",
    "course_repo = get_repo_name()\n",
    "course_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dl-cci-idolized-jasmine-37'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_branch\n",
    "course_branch = get_branch()\n",
    "course_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sample application: AI-powered quiz generator\n",
    "\n",
    "Here is our sample application from the previous lesson that you will continue working on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema.output_parser import StrOutputParser\n",
      "\n",
      "delimiter = \"####\"\n",
      "\n",
      "\n",
      "def read_file_into_string(file_path):\n",
      "    try:\n",
      "        with open(file_path, \"r\") as file:\n",
      "            file_content = file.read()\n",
      "            return file_content\n",
      "    except FileNotFoundError:\n",
      "        print(f\"The file at '{file_path}' was not found.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "\n",
      "\n",
      "quiz_bank = read_file_into_string(\"quiz_bank.txt\")\n",
      "\n",
      "system_message = f\"\"\"\n",
      "Follow these steps to generate a customized quiz for the user.\n",
      "The question will be delimited with four hashtags i.e {delimiter}\n",
      "\n",
      "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
      "should only refer to the category.\n",
      "\n",
      "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
      "* Geography\n",
      "* Science\n",
      "* Art\n",
      "\n",
      "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are in the quiz bank below:\n",
      "\n",
      "#### Start Quiz Bank\n",
      "{quiz_bank}\n",
      "\n",
      "#### End Quiz Bank\n",
      "\n",
      "Pick up to two subjects that fit the user's category. \n",
      "\n",
      "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
      "\n",
      "* Only include questions for subjects that are in the quiz bank.\n",
      "\n",
      "Use the following format for the quiz:\n",
      "Question 1:{delimiter} <question 1>\n",
      "\n",
      "Question 2:{delimiter} <question 2>\n",
      "\n",
      "Question 3:{delimiter} <question 3>\n",
      "\n",
      "Additional rules:\n",
      "- Only include questions from information in the quiz bank. Students only know answers to questions from the quiz bank, do not ask them about other topics.\n",
      "- Only use explicit string matches for the category name, if the category is not an exact match for Geography, Science, or Art answer that you do not have information on the subject.\n",
      "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
      "\"\"\"\n",
      "\n",
      "\"\"\"\n",
      "  Helper functions for writing the test cases\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def assistant_chain(\n",
      "    system_message=system_message,\n",
      "    human_template=\"{question}\",\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
      "    output_parser=StrOutputParser(),\n",
      "):\n",
      "    chat_prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", system_message),\n",
      "            (\"human\", human_template),\n",
      "        ]\n",
      "    )\n",
      "    return chat_prompt | llm | output_parser"
     ]
    }
   ],
   "source": [
    "!cat app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first model graded eval\n",
    "Build a prompt that tells the LLM to evaluate the output of the quizzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "delimiter = \"####\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "eval_system_prompt = f\"\"\"You are an assistant that evaluates \\\n",
    "  whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the \\\n",
    "  format of Question N:{delimiter} <question N>?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate LLM response to make a first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "llm_response = \"\"\"\n",
    "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
    "\n",
    "Question 2:#### True or False: Water slows down the speed of light.\n",
    "\n",
    "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt for the evaluation (eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "eval_user_message = f\"\"\"You are evaluating a generated quiz \\\n",
    "based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {llm_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like \\\n",
    "a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, \\\n",
    "output N if the response does not look like a quiz.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to build the prompt template for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From langchain import a parser to have a readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect all pieces together in the variable 'chain'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "eval_chain = eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the 'good LLM' with positive response by invoking the eval_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function 'create_eval_chain'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 489
   },
   "outputs": [],
   "source": [
    "def create_eval_chain(\n",
    "    agent_response,\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "  delimiter = \"####\"\n",
    "  eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
    "  \n",
    "  eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
    "\"\"\"\n",
    "  eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])\n",
    "\n",
    "  return eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new response to test in the eval_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "known_bad_result = \"There are lots of interesting facts. Tell me more about what you'd like to know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "bad_eval_chain = create_eval_chain(known_bad_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response for wrong prompt\n",
    "bad_eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new create_eval_chain into the 'test_assistant.py' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from app import assistant_chain\n",
      "import os\n",
      "\n",
      "from dotenv import load_dotenv, find_dotenv\n",
      "\n",
      "_ = load_dotenv(find_dotenv())\n",
      "\n",
      "\n",
      "def test_science_quiz():\n",
      "    assistant = assistant_chain()\n",
      "    question = \"Generate a quiz about science.\"\n",
      "    answer = assistant.invoke({\"question\": question})\n",
      "    expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
      "    print(answer)\n",
      "    assert any(\n",
      "        subject.lower() in answer.lower() for subject in expected_subjects\n",
      "    ), f\"Expected the assistant questions to include '{expected_subjects}', but it did not\"\n",
      "\n",
      "\n",
      "def test_geography_quiz():\n",
      "    assistant = assistant_chain()\n",
      "    question = \"Generate a quiz about geography.\"\n",
      "    answer = assistant.invoke({\"question\": question})\n",
      "    expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
      "    print(answer)\n",
      "    assert any(\n",
      "        subject.lower() in answer.lower() for subject in expected_subjects\n",
      "    ), f\"Expected the assistant questions to include '{expected_subjects}', but it did not\"\n",
      "\n",
      "\n",
      "def test_decline_unknown_subjects():\n",
      "    assistant = assistant_chain()\n",
      "    question = \"Generate a quiz about Rome\"\n",
      "    answer = assistant.invoke({\"question\": question})\n",
      "    print(answer)\n",
      "    # We'll look for a substring of the message the bot prints when it gets a question about any\n",
      "    decline_response = \"I'm sorry\"\n",
      "    assert (\n",
      "        decline_response.lower() in answer.lower()\n",
      "    ), f\"Expected the bot to decline with '{decline_response}' got {answer}\""
     ]
    }
   ],
   "source": [
    "!cat test_assistant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from app import assistant_chain\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema.output_parser import StrOutputParser\n",
      "import pytest\n",
      "\n",
      "\n",
      "def create_eval_chain(\n",
      "    agent_response,\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
      "    output_parser=StrOutputParser(),\n",
      "):\n",
      "    delimiter = \"####\"\n",
      "    eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
      "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
      "\n",
      "    eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
      "  Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Response]: {agent_response}\n",
      "    ************\n",
      "    [END DATA]\n",
      "\n",
      "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
      "only evaluate if the data is in the expected format.\n",
      "\n",
      "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
      "\"\"\"\n",
      "    eval_prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", eval_system_prompt),\n",
      "            (\"human\", eval_user_message),\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    return eval_prompt | llm | output_parser\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def known_bad_result():\n",
      "    return \"There are lots of interesting facts. Tell me more about what you'd like to know\"\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def quiz_request():\n",
      "    return \"Give me a quiz about Geography\"\n",
      "\n",
      "\n",
      "def test_model_graded_eval(quiz_request):\n",
      "    assistant = assistant_chain()\n",
      "    result = assistant.invoke({\"question\": quiz_request})\n",
      "    print(result)\n",
      "    eval_agent = create_eval_chain(result)\n",
      "    eval_response = eval_agent.invoke({})\n",
      "    assert eval_response == \"Y\"\n",
      "\n",
      "\n",
      "def test_model_graded_eval_should_fail(known_bad_result):\n",
      "    print(known_bad_result)\n",
      "    eval_agent = create_eval_chain(known_bad_result)\n",
      "    eval_response = eval_agent.invoke({})\n",
      "    assert (\n",
      "        eval_response == \"Y\"\n",
      "    ), f\"expected failure, asserted the response should be 'Y', \\\n",
      "    got back '{eval_response}'\""
     ]
    }
   ],
   "source": [
    "# Command to see the content of the file\n",
    "!cat test_release_evals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** if you want to inspect the config run `!head circle_config.yml`\n",
    "\n",
    "Command: !cat circle_config.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push new files into CircleCI's Git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading app.py\n",
      "uploading test_assistant.py\n",
      "uploading test_release_evals.py\n"
     ]
    }
   ],
   "source": [
    "from utils import push_files\n",
    "push_files(course_repo, \n",
    "           course_branch, \n",
    "           [\"app.py\",\n",
    "            \"test_release_evals.py\",\n",
    "            \"test_assistant.py\"],\n",
    "           config=\"circle_config.yml\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the Release Evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from utils import trigger_release_evals\n",
    "trigger_release_evals(course_repo, \n",
    "                      course_branch, \n",
    "                      [\"app.py\",\n",
    "                       \"test_assistant.py\",\n",
    "                       \"test_release_evals.py\"],\n",
    "                      cci_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
